\section{Generalization of models} \label{sec:generalization}
Generalization refers to a model's ability to perform well on new, unseen data, rather than just the data it was trained on. This is crucial because the ultimate goal of a model is to make accurate predictions on real-world data, not just the training dataset. For data driven models overfitting can be a problem. The model learns the training data too well, including noise and outliers so that it performs poorly on new data. The opposite, underfitting the data can also be a problem. The model is too simple to capture the underlying patterns in the data, leading to poor performance on both training and new data.  

The biasâ€“variance tradeoff is a fundamental concept in understanding generalization. A model with high bias (underfitting) makes strong assumptions about the data, while a model with high variance (overfitting) is too sensitive to the training data. The goal is to find a balance between bias and variance to achieve good generalization. Cross validation is one way to prevent overfitting, which is further explained in the next section.